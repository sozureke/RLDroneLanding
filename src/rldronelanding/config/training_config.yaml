algorithm: ppo
total_timesteps: 5000000
learning_rate: 0.0003               
learning_rate_schedule: linear       
gamma: 0.995
batch_size: 128
n_steps: 1024
ent_coef: 0.03
clip_range: 0.2
gae_lambda: 0.95
policy: "MlpPolicy"
policy_kwargs:
  net_arch: [256, 256]
  ortho_init: true
log_dir: "logs/ppo/"
model_save_path: "models/ppo_drone"
render: false
enable_wind: false
enable_platform_motion: false
